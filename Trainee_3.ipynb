{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Classificação binária para prever fraudes"
      ],
      "metadata": {
        "id": "JjvdMabAT8C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparação dos dados"
      ],
      "metadata": {
        "id": "K52sCwF_UCme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui vamos separar os dados que queremos para análise em x e o dado que queremos prever em y."
      ],
      "metadata": {
        "id": "kWGzhaxRUGqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "j6UyRTc8UHrP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = pd.read_csv('creditcard.csv')"
      ],
      "metadata": {
        "id": "Sp3lPFiJUpwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizando os dados, separando a Class em y\n",
        "X = dt.drop(columns=['Class']).values\n",
        "y = dt['Class'].values"
      ],
      "metadata": {
        "id": "tIcWxMCsUKoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividindo os dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2 , random_state=42)"
      ],
      "metadata": {
        "id": "Q2K__Z0JUNV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Padronizando os recursos (Z-score)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NHATFW8EUOf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialização dos Parâmetros"
      ],
      "metadata": {
        "id": "utbwz5NHUPff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Nessa etapa é selecionado o número de camadas ocultas, os seus pesos e o seus vieses. É importante que os pesos sejam escolhidos de forma aleatória para que não se obtenha resultados indesejados. Iniciando os pesos aleatoriamente se permite que a rede neural explore diferentes soluções."
      ],
      "metadata": {
        "id": "jZvtyk07URzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "\n",
        "learning_rate_values = [0.001,0.01,0.1]\n",
        "num_epochs_values = [100,500,1000,2000]\n"
      ],
      "metadata": {
        "id": "ezhetXqbUSwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como foi pedido, o modelo vai ser testado usando diferentes valores de épocas e learning rate. Se utiliza diferentes valores de épocas para que os pesos de cada neurônio converta cada vez mais para os resultados esperados."
      ],
      "metadata": {
        "id": "Dk_w41jcUU-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função de ativação"
      ],
      "metadata": {
        "id": "7th95P9NUWeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1-sig)"
      ],
      "metadata": {
        "id": "uRFqeoVIUXTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento do modelo"
      ],
      "metadata": {
        "id": "bZOGGEDiUY4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos iniciar o modelo e dar pesos aleatórios para cada neurônio. Faremos isso para épocas diferentes, o que vai fazer com que os pesos convertam cada vez mais para o valor esperado. O forward pass é realizado para obter as saídas do modelo e o backpropagation para calcular os gradientes."
      ],
      "metadata": {
        "id": "h2hlPAjJUZ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for learning_rate  in learning_rate_values:\n",
        "    for num_epochs in num_epochs_values:\n",
        "        #Inicialização dos pesos e vieses para a camada oculta\n",
        "        W_hidden = np.random.randn(input_size, hidden_size)\n",
        "        b_hidden = np.zeros((1,hidden_size))\n",
        "\n",
        "        #Inicialização dos pesos e vieses para a camada de saída\n",
        "        W_output = np.random.randn(hidden_size,output_size)\n",
        "        b_output = np.zeros((1,output_size))\n",
        "\n",
        "\n",
        "        for epoch in range (num_epochs):\n",
        "            #Passo para frente\n",
        "            hidden_activation = sigmoid(np.dot(X_train, W_hidden) + b_hidden)\n",
        "            output_activation = sigmoid(np.dot(hidden_activation,W_output)+ b_output)\n",
        "\n",
        "            #Erro\n",
        "            error = y_train.reshape(-1,1) - output_activation\n",
        "\n",
        "            #Backpropagation\n",
        "            d_output = error * output_activation * (1- output_activation)\n",
        "            d_hidden = d_output.dot(W_output.T)* hidden_activation * (1-hidden_activation)\n",
        "\n",
        "            #Atualização dos pesos e vieses\n",
        "            W_output += hidden_activation.T.dot(d_output)*learning_rate\n",
        "            b_output += np.sum(d_output, axis =0 , keepdims=True)*learning_rate\n",
        "            W_hidden += X_train.T.dot(d_hidden)*learning_rate\n",
        "            b_hidden += np.sum(d_hidden, axis =0 , keepdims = True)* learning_rate\n",
        "\n",
        "\n",
        "        #Avaliação do Modelo\n",
        "        hidden_activation_test = sigmoid(np.dot(X_test, W_hidden)+b_hidden)\n",
        "        output_activation_test = sigmoid(np.dot(hidden_activation_test,W_output)+ b_output)\n",
        "\n",
        "        predictions = (output_activation_test > 0.5).astype(int)\n",
        "\n",
        "        accuracy = np.mean(predictions == y_test.reshape(-1,1))\n",
        "        print(f'learning Rate: {learning_rate}, Num Epochs: {num_epochs}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "FIRa9374Ubx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A acuracia do modelo deu igual para todos os casos. Isso indica um erro no código que não consegui indentificar. Provavelmente o erro está no treinamento do modelo já que parece um erro de overfitting."
      ],
      "metadata": {
        "id": "Oy0q9R_RUggo"
      }
    }
  ]
}